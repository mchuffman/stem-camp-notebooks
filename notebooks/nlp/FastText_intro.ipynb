{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "FastText_intro.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cu-applied-math/stem-camp-notebooks/blob/master/notebooks/nlp/FastText_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83tzC4xQtyfK",
        "colab_type": "text"
      },
      "source": [
        "# Solving analogies with FastText\n",
        "\n",
        "In this section, we'll show how computers can learn analogies. For example,\n",
        "\n",
        "$$\n",
        "\\textrm{France is to Paris as Italy is to _______ ?}\n",
        "$$\n",
        "\n",
        "Paris is the capital of France, so this analogy is asking what the capital of Italy is. The answer is [Rome](https://www.britannica.com/place/Rome), of course! But we know this because of geography class. Can a computer learn this on its own? If so, what else can it learn? What will it have difficulty learning?\n",
        "\n",
        "To explore these questions, we'll use some pre-trained vectors from [fastText](https://fasttext.cc/), which was created by Facebook's Artificial Intelligence Lab. The goal of this project is to turn words into mathematical objects and see if mathematical relations can be used as a way to understand language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk2jd5PYtyfL",
        "colab_type": "text"
      },
      "source": [
        "Let's begin by importing the necessary functions. The FastText_helpers file was taken from [this article](https://medium.com/swlh/playing-with-word-vectors-308ab2faa519) to aid this analysis. You can take a closer look at this file to better understand the math behind this section if you're interested."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MafKGpN2vRGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If using Google Colab, we need to get some helper files\n",
        "! wget https://github.com/cu-applied-math/stem-camp-notebooks/raw/master/notebooks/nlp/FastText_helpers.py\n",
        "! wget https://github.com/cu-applied-math/stem-camp-notebooks/raw/master/notebooks/nlp/load.py\n",
        "! wget https://github.com/cu-applied-math/stem-camp-notebooks/raw/master/notebooks/nlp/main.py\n",
        "! wget https://github.com/cu-applied-math/stem-camp-notebooks/raw/master/notebooks/nlp/vectors.py\n",
        "! wget https://github.com/cu-applied-math/stem-camp-notebooks/raw/master/notebooks/nlp/word.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIosIwTstyfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from typing import List\n",
        "import vectors as v\n",
        "from vectors import Vector\n",
        "import FastText_helpers as ft\n",
        "from load import load_words\n",
        "import math\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwj0G3MbtyfP",
        "colab_type": "text"
      },
      "source": [
        "In this section, we will treat words as vectors, or a list of numbers. We won't go into the details of how these vectors are created for each word (that is what fastText does), but we will show how we can use these vectors to understand language better. Run the following cell to load in the words we'll use (97,193 words, to be exact) and their vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9GT3nG9v2C9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First, download the data. It's a bit large but not crazy: 650 MB or so (when compressed)\n",
        "# You may want to NOT do this, execute the next cell instead (for a smaller dataset)\n",
        "! wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "big_file = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuLdsBZG01f9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# or, download a subset of the data, which will make downloads and computing faster\n",
        "#! wget https://drive.google.com/file/d/1WCHFVHun6sF0_xI4PD1YBSnI4NVYFS5F/view?usp=sharing # Stephen uploaded to his google drive\n",
        "#!export FILEID=1WCHFVHun6sF0_xI4PD1YBSnI4NVYFS5F\n",
        "#!export FILENAME=wiki-news-300d-100k.vec\n",
        "# see https://medium.com/@acpanjan/download-google-drive-files-using-wget-3c2c025a8b99\n",
        "#!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=${FILEID}' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=${FILEID}\" -O ${FILENAME} && rm -rf /tmp/cookies.txt\n",
        "#!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1WCHFVHun6sF0_xI4PD1YBSnI4NVYFS5F' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1WCHFVHun6sF0_xI4PD1YBSnI4NVYFS5F\" -O wiki-news-300d-100k.vec && rm -rf /tmp/cookies.txt\n",
        "#!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=${FILEID}' -O ${FILENAME}\n",
        "#!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1WCHFVHun6sF0_xI4PD1YBSnI4NVYFS5F' -O wiki-news-300d-100k.vec\n",
        "\n",
        "# Instead, try this: (see https://stackoverflow.com/a/50670037)\n",
        "!pip install gdown\n",
        "!gdown https://drive.google.com/uc?id=1WCHFVHun6sF0_xI4PD1YBSnI4NVYFS5F\n",
        "big_file = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKs-QccowYqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract the zip file (now 2.2 GB)\n",
        "if big_file == True\n",
        "  !unzip wiki-news-300d-1M.vec.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31XweVT0tyfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# And load the data into our python session\n",
        "if big_file == True:\n",
        "  print(\"Loading big 2.2 GB file\")\n",
        "  words = load_words('wiki-news-300d-1M.vec')\n",
        "else:\n",
        "  print(\"Loading medium 216 MB file\")\n",
        "  words = load_words('wiki-news-300d-100k.vec') \n",
        "  #words = load_words('/data/FastText/wiki-news-300d-100k.vec')  # old location"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBYKfEQNtyfS",
        "colab_type": "text"
      },
      "source": [
        "Now that we have our data loaded to memory with the variable `words`, let's take a closer look at what the data looks like. We can look at a word by typing `words[k]` for some number, $k$, between 0 and 97191. This will return the $\\textrm{k}^{\\textrm{th}}$ word, along with its associated vector. The access just the word, type `word[k].text` and to access the vector, type `word[k].vector` \n",
        "\n",
        "You can also search the \"word\" variable to see if it has a certain word using the `ft.find_word(words,str)` command. If the string is in words, then it will return the word, if not it will let us know that that word wasn't found.\n",
        "\n",
        "**Q**: How long of a vector is used for each word?\n",
        "\n",
        "**Q**: Find the vector for the word \"Paris\". Is it the same vector used for \"paris\"?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuFNMSzvtyfT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "### TODO : find the length of a vector for each word in our \"words\" list\n",
        "\n",
        "\n",
        "### TODO : find the vectors that corresponds to the words \"Paris\" and \"paris\". Are they the same?\n",
        "# You may wish to use the vector functions, e.g., v.dot(x,y),  v.normalize(x), v.cosine_similarity_normalized(x,y)\n",
        "# or the FastText_helpers (imported as ft)\n",
        "dir(ft)  # shows you available commands"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LATDorGNtyfV",
        "colab_type": "text"
      },
      "source": [
        "We can visualize some of these vectors by plotting the first two entries of the 300-length array. Let's see what this looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "213GqJ8wtyfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using PCA is a common alternative to reduce dimensions\n",
        "#  as well as tSNE\n",
        "for i in range(0,151,10):\n",
        "    x,y = words[i].vector[0:2]\n",
        "    wordtext = words[i].text\n",
        "    plt.scatter(x,y,c=\"red\")\n",
        "    plt.annotate(wordtext,xy=(x,y),xytext=(x+.0005,y))\n",
        "    plt.xlabel(\"axis 0\")\n",
        "    plt.ylabel(\"axis 1\")\n",
        "    plt.title(\"Random words\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozzx1yA-tyfY",
        "colab_type": "text"
      },
      "source": [
        "Note that these are only 2 of the 300 dimensions of our vector, so it's not a perfect representation of our words.\n",
        "\n",
        "**Q**: Do you see any patterns among clusters of words?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXuwq6JHtyfZ",
        "colab_type": "text"
      },
      "source": [
        "To get a better sense of how similar two words are, we have the function called `ft.cosine_similarity(vector1,vector2)`. The [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity#Definition) finds the angle between the vectors for two words and determines how similar they are (it returns $1$ if they are very similar and $-1$ if they are not very similar). The following code will compute the similarity between two words given by you! Try out some pairs!\n",
        "\n",
        "**Q**: What is the cosine similarity between any word and itself?\n",
        "\n",
        "**Q**: Do opposites (for example: hot and cold, or rich and poor) have high or low cosine similarities? Does your answer surprise you?\n",
        "\n",
        "**Q**: The cosine similarity looks at the angle between two vectors. Are there any other ways in which we could compare two vectors? Try your own method of comparison below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jIEa6MBtyfZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f2544bd-c4df-4eb1-de9b-b4880fa29462"
      },
      "source": [
        "word1 = \"Italy\";#### insert word here\n",
        "word2 = \"France\";#### insert word here\n",
        "\n",
        "word1_w = ft.find_word(words,word1) ###Find these words in our list of words\n",
        "word2_w = ft.find_word(words,word2)\n",
        "\n",
        "#compute the similarity between these words\n",
        "similarity = ft.cosine_similarity(word1_w.vector,word2_w.vector)\n",
        "\n",
        "print(\"The cosine similarity between these two words is \" + str(similarity))\n",
        "\n",
        "#Compute your own similarity method\n",
        "#similarity_2 = ...\n",
        "#print(\"My own similarity measure between these two words is \" + str(similarity_2))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The cosine similarity between these two words is 0.7184823438635273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_Nv4ANhtyfe",
        "colab_type": "text"
      },
      "source": [
        "Now we have a way to describe how similar any two words are. A natural next question is how we can find the words closest to any particular word? We can do this with the `ft.print_related(words,word)` function, which takes the given word, compares it to all words in our list, and finds which have the highest level of cosine similarity. \n",
        "\n",
        "**Q**: Which words are most similar to the following words: \"president\",\"Italy\",\"pizza\",,\"sew\",\"sewer\". Do any of these so-called similar words surprise you?\n",
        "\n",
        "**Q**: Look for the words similar to \"principle\". Did you find the similar words you were expecting? Why or why not?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihldcu58tyfe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e0bcc03e-eaa6-4f3d-82a1-5fe9d573c5a5"
      },
      "source": [
        "ft.print_related(words,\"Italy\")\n",
        "\n",
        "# Brother - Sister = gender\n",
        "# King - Queen = Brother - Sister\n",
        "# King - Queen + Sister = ... ? (Brother?)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rome, Tuscany, France, Sicily, Germany, Naples, Italian\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZsDHV8rtyfh",
        "colab_type": "text"
      },
      "source": [
        "Now we're ready to start solving analogies!!! Let's return to our original question, \n",
        "\n",
        "$$\n",
        "\\textrm{France is to Paris as Italy is to _______ ?}\n",
        "$$\n",
        "\n",
        "Recall that now, we are thinking about these words as points in space and not as countries and their capitals. It turns out that in this way, we can assume these words have a simple relation to solve the analogy. Breaking our vectors into $(x,y)$ points, we may assume they look something like this (run the following cell):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZAmcNsCtyfh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pt_matrix = [[0,0],[1,2],[2,0],[3,2]]\n",
        "word_list = [\"Paris\",\"France\",\"Italy's capital\",\"Italy\"]\n",
        "for i in range(0,4):\n",
        "    x,y = pt_matrix[i]\n",
        "    plt.scatter(x,y,c=[\"red\" if i<2 else \"green\"])\n",
        "    plt.annotate(word_list[i],xy=(x,y),xytext=(x+.07,y),fontsize=14)\n",
        "    if i%2==0:\n",
        "        plt.arrow(x,y,.975,1.95,color=\"black\",lw = 2,head_width = .05,\n",
        "          length_includes_head=True)\n",
        "        \n",
        "plt.xlim((-.25,3.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEeT1hRGtyfk",
        "colab_type": "text"
      },
      "source": [
        "If this schematic of the relation between countries and their capitals is true, then the vectors connecting them should be indentical! This would suggest we have the relation:\n",
        "\n",
        "$$\n",
        "\\textrm{France - Paris} \\approx \\textrm{Italy - Italy's capital.}\n",
        "$$\n",
        "\n",
        "(Remember that we are thinking of these words as vectors with FastText, so it makes sense to add and subtract words). So, if we want to know the capital of Italy, we can assume:\n",
        "\n",
        "$$\n",
        "\\textrm{Italy's capital} \\approx  \\textrm{Italy - France + Paris}\n",
        "$$\n",
        "\n",
        "More generally, we can think\n",
        "\n",
        "$$\n",
        "\\textrm{Capital city 2} \\approx  \\textrm{Country 2 - Country 1 + Capital city 1}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JBE_pPCtyfk",
        "colab_type": "text"
      },
      "source": [
        "So now to use FastText to try and answer our analogy, we can look for the word whose is vector closest to the vector given by Italy - France + Paris. This is exactly what the function `ft.print_analogies(left2,left1,right2,words)` does: it loops through each vector in our list and determines which word is closest to the above sum. Does it predict the capital of Italy?\n",
        "\n",
        "**Q**: Using France and Paris as our analogy, what other capital cities can FastText predict (You can find a list of capitals [here](https://www.thoughtco.com/capitals-of-every-independent-country-1434452)). What countries does fastText have trouble predicting? What trends do you notice when the wrong capital cities are predicted?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gs4onh_ftyfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ft.print_analogy(\"France\",\"Paris\",\"Ecuador\",words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RG3hVDFttyfm",
        "colab_type": "text"
      },
      "source": [
        "notes: Fasttext (as is) cannot predict the capitals of:\n",
        " Cambodia, Canada, Mexico, USA, and many others"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aF7soMJAtyfn",
        "colab_type": "text"
      },
      "source": [
        "Let's explore some more analogies and how well fastText has learned them:\n",
        "\n",
        "Let's start with some plurals: If we provide the singular-plural versions of a word, such as horse and horses, what are some words whose plurals we can learn? \n",
        "\n",
        "**Q**: Plural words are one of the subtleties of the English language. Usually, we only need to add an \"s\" to the end of the word to get its plural. Can fastText understand that some words have different forms of plurals?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPTh7Dhztyfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_compare = [\"goose\",\"cactus\",\"bacterium\",\"child\",\"foot\",\"man\",\"mouse\",\"woman\"]\n",
        "\n",
        "for word_comp in to_compare:\n",
        "    ft.print_analogy(\"horse\",\"horses\",word_comp,words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUOVW29ctyfq",
        "colab_type": "text"
      },
      "source": [
        "What about opposites? Can Fasttext predict opposite words if it's given an example?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9RjyF1vtyfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_compare = [\"hot\",\"rich\",\"old\",\"fun\",\"boring\",\"boy\",\"easy\"]\n",
        "\n",
        "for word_comp in to_compare:\n",
        "    ft.print_analogy(\"good\",\"bad\",word_comp,words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3PdfTyHtyfs",
        "colab_type": "text"
      },
      "source": [
        "Can you come up with some analogies on your own to test fastText with??"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6icmw8mtyft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
